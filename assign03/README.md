# CS6650 Fall 2021 Assignment 2

## <a name="github">**GitHub**</a>:

**https://github.com/timegao/cs6650-fa21/tree/main/assign03**

&nbsp;

## **Submission Requirements**:

Submit your work to Canvas Assignment 3 as a pdf document. The document should contain:

> [The URL for your git repo](#github).

> [Create a new folder for your Assignment 3 server code](https://github.com/timegao/cs6650-fa21/tree/main/assign03/src/main/java/server)

> A short description of your [database designs and deployment topologies on AWS](#design)

> [Test runs](#results) ([command windows](#command), [RMQ management windows showing queue size, send/receive rates](#SQS)) 128, 256 client threads for Step 1 and 2.

> [Test run for 128, 256 clients](#results) ([command windows](#command), [RMQ management windows showing queue size, send/receive rates](#SQS)) with both microservices consuming and writing to the database.

> A brief explanation of your [mitigation strategy](#mitigation) and [results](#results) with 128 256 clients to show their effects.

&nbsp;

## **Introduction**:

Assignment description: https://gortonator.github.io/bsds-6650/assignments-2021/Assignment-3

This assignment builds on assignment 2. The main aim is to persist the tuples generated by the your client.

It’s likely that the AWS restrictions on the number of instances you can start will mean you can’t use the load balancer. Feel free to increase capacity of servers - just report what you used for gathering result sand watch your $$s!

&nbsp;

## <a name="design">**Design**</a>:

### Step 1 - Adding a Skier Microservice

In assignment 2, your consumer created an in-memory hash map to skier and lift ride information.

Your first task is to modify the consumer to persist the results to a database.

You are free to choose any database you like. AWS RDS instances like MySQL, DynamoDB, MongoDB, Redis - all good candidates.

The data model you design in the database should enable queries like:

“For skier N, how many days have they skied this season?”

“For skier N, what are the vertical totals for each ski day?”

“For skier N, show me the lifts they rode on each ski day”

Whatever database you choose, the challenge is to write to the database ideally as fast as you can consume messages from RabbitMQ. This may be challenging based on the EC2 resources you choose, so experiment required. Run some experiments.

[Test](#results) this configuration by reporting the same results as assignment 2 for 128, 256 clients. Feel free to empty the database between tests. It might help ;)

### Step 2 - Adding a Resorts Microservice

Next, add a new consumer and database for storing data pertinent to the ski resort. The consumer should ingest the same data as the Skier Microservice. The data model should be designed to answer questions like:

“How many unique skiers visited resort X on day N?”
“How many rides on lift N happened on day N?”
“On day N, show me how many lift rides took place in each hour of the ski day”
Again, test to see if you can write to the database as quickly as the data is published to RMQ.

[Test](#results) this configuration (without skier microservice) by reporting the same results as assignment 2 for 128, 256 clients

### Step 3 - Experiments

First, run [tests](#results) for 128 and 256 with both microservices and see what happens.

It’s likely you will see significant backlogs in your queues, servlet and maybe even consumers.

If you do, you have two choices:

> Increase capacity - this means deploying more than free tier instances. Watch the $$s.

> Introduce throttling - you could do this e.g. in the client by introducing a throughput-based circuit breaker with exponential backoffs in client POSTs and/or RMQ posts/configuration
You don’t have to do both. But your aim is to try and deliver more stable throughput and eliminate client errors that may occur when no mitigation measures were used.

### <a name="classes">**Classes**</a>

#### client

#### server/producer

#### consumer

### <a name="dependencies">**Dependencies**</a>

### <a name="commands">**Commands**</a>

&nbsp;

## <a name="results">**Results**</a>

&nbsp;

### Observations

There appears to be thermal limitations on the AWS CPU's where running the same task one after the other could vastly  affect the results.
This is understandable because chips inevitably get hot after running full throttle for 10+ minutes at a time,
and the cooling solution for these low power chips might not be very good.
t2.micro chips are from 2013, which is ancient in today's terms.
t3.micro is much better suited for multi-threaded web servers with 2 cores instead of 1 for t2.micro and 9.31 GB/s memory bandwith instead of t2.micro's 15.5 GB/s.
Unfortunately AWS doesn't let you create EC2 instances from AMI's. For the very result, I found it better to switch EC2 instances after every run.
Even after waiting for as long as 30 minutes, the chips have not cooled down enough for an effective second run.

&nbsp;

## <a name="uml">**UML**</a>

### producer

### consumer